{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9KvrrZzuqGS"
      },
      "source": [
        "## Project 1. Nested Monte-Carlo for expected loss\n",
        "\n",
        "\n",
        "#### $\\blacktriangleright$ Setting\n",
        "We consider a Black-Scholes model at discrete dates $t_i$:\n",
        "\n",
        "$$\n",
        "S_{t_i} = S_0 \\, e^{\\sigma \\, W_{t_i} - \\frac 12 \\sigma^2 t_i},\n",
        "\\qquad t_0 < t_1 < \\dots < t_n,\n",
        "$$\n",
        "\n",
        "which we use to generate option prices (note we consider zero interest rate $r=0$). \n",
        "\n",
        "In particular, the price at time $t_1$ of a Put option with maturity $t_2$ and strike price $K$ is given by \n",
        "\n",
        "$$\n",
        "P(t_1, S_{t_1}) = \\mathbb E \\bigl[ (K - S_{t_2})^+ \\big| S_{t_1} \\bigr]\n",
        "$$\n",
        "\n",
        "where $x^+$ denotes the positive part of $x$; the explicit Black-Scholes formula is of course available for the function $P(t,S)$\n",
        "\n",
        "$$\n",
        "P(t,S) = K \\mathcal N \\biggl(\\frac{\\ln \\frac K {S}}{\\sigma \\sqrt{t_2 - t}} + \\frac 12 \\sigma \\sqrt{t_2 - t} \\biggr)\n",
        "- S \\, \\mathcal N \\biggl(\\frac{\\ln \\frac K {S}}{\\sigma \\sqrt{t_2 - t}} - \\frac 12 \\sigma \\sqrt{t_2 - t} \\biggl),\n",
        "\\qquad t \\le t_2, \\ S > 0\n",
        "$$\n",
        "\n",
        "where $\\mathcal N$ is the standard gaussian cdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYEEtJufuqGU"
      },
      "source": [
        "We wish to apply Monte-Carlo simulation to the evaluation of the _nested_ expectation\n",
        "\n",
        "$$\n",
        "I = \\mathbb E \\Bigl[\\Bigl(P(t_1, S_{t_1}) - P(t_0, S_0) \\Bigr)^+ \\Bigr]\n",
        "$$\n",
        "\n",
        "which represents the _expected loss_ (or exposure) at time $t_1$ of a short position on the put option of maturity $t_2$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8QCI08_uqGU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as sps\n",
        "import numpy.linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEU2tTafuqGW"
      },
      "source": [
        "####  1. Benchmark value (semi-explicit in this case)\n",
        "\n",
        "In the framework of the model above, it is possible to evaluate the nested expectation $I$ using numerical evaluation of a 1D integral with a deterministic quadrature formula, since\n",
        "\n",
        "$$\n",
        "I = \\int_{\\mathbb R} \\Bigl(P \\bigl(t_1, S_0 \\, e^{\\sigma \\, \\sqrt{t_1} \\, y \\, - \\, \\frac 12 \\sigma^2 t_1} \\bigr) - P(t_0, S_0) \\Bigr)^+\n",
        "\\frac{e^{-\\, y^2/2}}{\\sqrt{2 \\pi}} \\, dy\n",
        "$$\n",
        "\n",
        "and the integrand function is explicit.\n",
        "\n",
        "We can numericall evaluate the integral on the right hand side using the function `scipy.integrate.quad` (we suggest to check out the input and output formats of this function on the online documentation).\n",
        "\n",
        "We will use this value as a benchmark value for $I$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by initialing the parameters."
      ],
      "metadata": {
        "id": "qPNjxJDheg_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5xs_UiMaQjP"
      },
      "outputs": [],
      "source": [
        "# Option parameters\n",
        "t_2 = 2 #maturity\n",
        "K = 100 #strike price\n",
        "S_0 = 100 #initial stock price\n",
        "\n",
        "# Model parameter\n",
        "sigma = 0.3 #volatility\n",
        "\n",
        "# Intermediate date\n",
        "t_1 = t_2/2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define a function that computes the Black-Scholes price at a given time t for put option with maturity T.\n",
        "\n",
        "More precisely, we first compute the time difference deltaT between T and t, and then calculate sigmaSqrtDeltaT as the product of the volatility sigma and the square root of deltaT.\n",
        "\n",
        "Next, we compute the parameter d by dividing the natural logarithm of the strike price K by S (the current stock price), multiplied by sigmaSqrtDeltaT, and adding sigmaSqrtDeltaT divided by 2.\n",
        "\n",
        "Finally, the return the put option price $P(S,t)$ calculated using the Black-Scholes formula, which is K times the cumulative distribution function of d minus S times the CDF of d minus sigmaSqrtDeltaT."
      ],
      "metadata": {
        "id": "8_WhAxoIddwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfqSgp9paUVE"
      },
      "outputs": [],
      "source": [
        "def putPrice(t, S, T, K, sigma):\n",
        "    \"\"\"\n",
        "    Black-Scholes price at time t for put option with maturity T.\n",
        "    \"\"\"\n",
        "    deltaT = T - t\n",
        "    sigmaSqrtDeltaT = sigma * np.sqrt(deltaT)\n",
        "    \n",
        "    d = np.log(K/S) / sigmaSqrtDeltaT + sigmaSqrtDeltaT/2\n",
        "    \n",
        "    return K * sps.norm.cdf(d) - S * sps.norm.cdf(d - sigmaSqrtDeltaT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBFRTPqFacsj"
      },
      "outputs": [],
      "source": [
        "\n",
        "##############################################################\n",
        "# TO DO: evaluate the initial price P(0, S_0)\n",
        "\n",
        "price_at_zero = putPrice(t=0, S=S_0, T=t_2, K=K, sigma=sigma) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a function that is used for integration using scipy.integrate.quad. It calculates the loss at $t_{1}$ by calling putPrice with the corresponding $S_{t_{1}}$ value. It then computes the positive part of the loss, multiplies it by the normal density of a standard normal distribution, and returns the result which is the quantity under the integral in the definition of $I$ ."
      ],
      "metadata": {
        "id": "oNU_0ddgdoBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4CJGNyDuqGW"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Evaluation of the benchmark value with scipy.integrate.quad \n",
        "##############################################################\n",
        "\n",
        "def to_integrate(y):\n",
        "    \n",
        "    S_t_1 = S_0 * np.exp(sigma*np.sqrt(t_1) * y - 0.5*sigma*sigma*t_1)\n",
        "    \n",
        "    loss_t_1 = putPrice(t=t_1, S=S_t_1, T=t_2, K=K, sigma=sigma) - price_at_zero\n",
        "\n",
        "    positivePart = np.maximum(loss_t_1, 0.)\n",
        "\n",
        "    density = np.exp(-0.5*y*y) / np.sqrt(2*np.pi)\n",
        "\n",
        "    return positivePart * density"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use the quad function from scipy.integrate and evaluates the integral of to_integrate over the range $]-∞, ∞[$. The result is assigned to benchmark_value, and it represents the expected loss at $t_1$."
      ],
      "metadata": {
        "id": "cHbdUPe9eD9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-B8VBH6arOu",
        "outputId": "df8f28e5-b9b9-4b57-ef57-9232b4868fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Benchmark value for the expected loss at time t_1: 5.296\n"
          ]
        }
      ],
      "source": [
        "###################################################\n",
        "# To Do:\n",
        "# + Import the function quad from scipy.integrate\n",
        "# + Evalue the value of the integral I\n",
        "\n",
        "from scipy.integrate import quad\n",
        "\n",
        "benchmark_value = quad(to_integrate, a= -np.inf, b =np.inf)[0]\n",
        "\n",
        "print(\"\\n Benchmark value for the expected loss at time t_1: %1.3f\" %benchmark_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyo2FEC0uqGZ"
      },
      "source": [
        "#### 2. Nested Monte-Carlo estimator\n",
        "\n",
        "In the definition of $I$, we can recognize a nested expectation of the form\n",
        "\n",
        "$$\n",
        "\\mathbb E \\bigl[ g \\bigl( E[f(X,Y)|X] \\bigr)  \\bigr]\n",
        "$$\n",
        "\n",
        "where $f,g$ are two given functions and $X$ and $Y$ are two independent random variables, since\n",
        "\n",
        "$$\n",
        "I = \\mathbb E \\biggl[ \n",
        "\\biggl( \\mathbb E \\Bigl[ \\Bigl(K - S_{t_1} e^{\\sigma \\, (W_{t_2} - W_{t_1}) - \\frac 12 \\sigma^2 (t_2 - t_1)} \\Bigr)^+ \\Big| S_{t_1} \\Bigr] - P(t_0, S_0) \\biggr)^+\n",
        "\\biggr]\n",
        "$$\n",
        "\n",
        "and the Brownian increment $W_{t_2} - W_{t_1}$ is independent of $S_{t_1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihFed09XuqGa"
      },
      "source": [
        "We consider\n",
        "    \n",
        "+ $M$ i.i.d. samples $S_m$ of $S_{t_1}$\n",
        "\n",
        "    \n",
        "+ $M \\times N$ i.i.d. samples of the Brownian increment , denoted $\\Delta W_{m,n}$\n",
        "\n",
        "The idea behind the __Nested Monte-Carlo estimator__ is the following: for every value $S_m$ sampled at $t_1$, we use the vector of $N$ samples $(\\Delta W_{m,n})_{n = 1, \\dots, N}$ of the Brownian increment to approximate the _inner_ conditional expectation in the expression above.\n",
        "Then, we have to take the outer mean with respect to the simulated values of $S_{t_1}$.\n",
        "\n",
        "The resulting Nested Monte-Carlo estimator is\n",
        "\n",
        "$$\n",
        "I_{M,N} = \\frac 1 M \\sum_{m=1}^M\n",
        "\\biggl(\n",
        "\\frac 1 N \\sum_{n=1}^N\n",
        "\\Bigl(K - S_m \\, e^{\\sigma \\, \\Delta W_{m,n} \\, - \\, \\frac 12 \\, \\sigma^2 (t_2 - t_1)} \\Bigr)^+\n",
        "- P(t_0, S_0)\n",
        "\\biggr)^+.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yllq6m5WuqGb"
      },
      "source": [
        "#### $\\blacktriangleright$ Implementation: \n",
        "\n",
        "In the cell below, complete the function `NestedEstimator` so that it outputs one single sample of the estimator $I_{M,N}$.\n",
        "\n",
        "Then, generate a sample of $k$ i.i.d. values of the estimator $I_{M,N}$, choosing\n",
        "\n",
        "+ first $M=N$ (and for example $M=1000$)\n",
        "\n",
        "+ then a smaller value of $N$, of the order of $\\sqrt M$.\n",
        "\n",
        "In both case, give an estimate of the RMSE \n",
        "\n",
        "$$\n",
        "\\sqrt{ \\mathbb E \\Bigl[ \\bigl(I - I_{M,N} \\bigr)^2 \\Bigr]}\n",
        "$$\n",
        "\n",
        "and plot the histogram of the $k$ simulated values of the estimator $I_{M,N}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Nested Monte Carlo method is to estimate quantities that involve nested expectations or conditional expectations, where each level of expectation requires independent random samples. \n",
        "\n",
        "In our problem, the Nested Monte Carlo method is applied to estimate the value of $I$, which involves nested expectations. The outer expectation involves the conditional expectation of a function $g$ evaluated at the inner expectation of a function $f$, which depends on independent random variables X and Y. This method, as said before, gives us $I_{M,N}$."
      ],
      "metadata": {
        "id": "hzYpYaqVj3l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function NestedEstimator to calculate one sample of the Nested Monte-Carlo estimator denoted as $I_{M,N}$. This estimator is used to estimate a certain value involving random variables using nested Monte-Carlo simulations.\n",
        "\n",
        "The function takes several parameters as input: t_1 and t_2 (representing the initial and final times), K (the strike price), S_0 (the initial stock price), sigma (the volatility), M (the number of samples for S_t_1), and N (the number of samples for the Brownian increment).\n",
        "\n",
        "First, we generate M i.i.d. samples of a standard normal distribution using np.random.normal, which are stored in the variable G. Then, we calculate the values of $S_{t_{1}}$ for each sample.\n",
        "\n",
        "Next, we generate M * N i.i.d. samples of the Brownian increment, denoted as Delta_W, using np.random.normal. The size of Delta_W is (M, N), where M represents the number of  $S_{t_{1}}$ samples, and N represents the number of Brownian increment samples.\n",
        "\n",
        "The Nested Monte-Carlo estimation process begins by initializing the outer sum, represented by the variable outerSum. Then, it iterates over each sample of  $S_{t_{1}}$.\n",
        "\n",
        "For each $S_{t_{1}}$sample, we compute the corresponding $S_{2}$ values using the exponential factor calculated from Delta_W. The inner Monte-Carlo mean is approximated by taking the average of the maximum of $(K - S_{2})$ and 0 over N samples.\n",
        "\n",
        "The outer sum is updated by adding the maximum of (innerMean - price_at_zero) and 0 for each iteration. Finally, the Nested Monte-Carlo estimator $I_{M,N}$ is obtained by dividing the outerSum by M and stored in the variable Nested_estimator, which is returned as the output of the function."
      ],
      "metadata": {
        "id": "TBk9DIKdluG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GGtBT8KuqGb"
      },
      "outputs": [],
      "source": [
        "def NestedEstimator(t_1, t_2, K, S_0, sigma, M, N):\n",
        "    \"\"\"\n",
        "    This function outputs one sample of the \n",
        "    Nested MC estimator I_{M,N}\n",
        "    \"\"\"\n",
        "    #######################################\n",
        "    # M i.i.d. samples of S_t_1\n",
        "    G = np.random.normal(loc=0, scale=1, size=M)\n",
        "    \n",
        "    S_1 = S_0 * np.exp(sigma*np.sqrt(t_1)*G - 0.5*sigma*sigma*t_1)\n",
        "    \n",
        "    #######################################\n",
        "    # To Do: simulate M*N i.i.d. samples \n",
        "    # of the Brownian increment Delta W\n",
        "    Delta_W = np.sqrt(t_2 - t_1) * np.random.normal(loc=0, scale=1, size=(M,N))\n",
        "    \n",
        "    ##############################################\n",
        "    # We evaluate the NestedMonte-Carlo estimator\n",
        "    ##############################################\n",
        "    \n",
        "    # This variable will contain the outer sum over {m = 1, ..., M}\n",
        "    outerSum = 0.\n",
        "    \n",
        "    for m in range(M):\n",
        "        # We work with one-dimensional arrays of size N\n",
        "        exponential_factor = np.exp(sigma * Delta_W[m] - 0.5*sigma*sigma*(t_2 - t_1))\n",
        "        \n",
        "        S_2 = S_1[m] * exponential_factor\n",
        "        \n",
        "        ##################################################\n",
        "        ## To Do: implement the inner Monte-Carlo mean 1/N sum_{n = 1, ..., N}\n",
        "        innerMean = 1/N * np.sum(  np.maximum(K - S_2, 0.) )\n",
        "        \n",
        "        ##################################################\n",
        "        ## To do: update the outer sum_{m = 1, ..., M}\n",
        "        outerSum += np.maximum(innerMean - price_at_zero, 0.)\n",
        "    \n",
        "    Nested_estimator = outerSum / M\n",
        "    \n",
        "    return Nested_estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate now two different cases for M and N and compare them."
      ],
      "metadata": {
        "id": "JGup0UiInIsQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbNmaX7ua3d1"
      },
      "source": [
        "**First case:** \n",
        "\n",
        "we will generate a sample of  $k = 1000 $  i.i.d. values of the estimator $I_{M,N}$ , choosing\n",
        "$$M = N = 1000$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "jzROZqR30ZbM",
        "outputId": "1107d6c6-ac99-4187-a54a-b9c84f8ebd9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 0.2790753815787428\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M = 1000\n",
        "N = 1000\n",
        "k = 1000\n",
        "\n",
        "# Generate k samples of the Nested MC estimator\n",
        "I_MN = [NestedEstimator(t_1, t_2, K, S_0, sigma, M, N) for _ in range(k)]\n",
        "\n",
        "# Estimate the RMSE\n",
        "rmse = np.sqrt(np.mean([(benchmark_value - sample)**2 for sample in I_MN]))\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Plot the histogram of the k samples\n",
        "plt.hist(I_MN, bins=20)\n",
        "plt.title(\"Histogram of Nested MC estimator samples (M=N=1000)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2zGUSwFbcHV"
      },
      "source": [
        "**Second case:** \n",
        "\n",
        "we will generate a sample of  $k = 1000 $  i.i.d. values of the estimator $I_{M,N}$ , choosing\n",
        "$$M = N^2 = 1000$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "FO1UBNpp0zLT",
        "outputId": "ea0059c8-fbb5-4924-db6e-ae4bcfdfd210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 0.30936394076588813\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M = 1000\n",
        "N = int(np.sqrt(M))\n",
        "k = 1000\n",
        "\n",
        "# Generate k samples of the Nested MC estimator\n",
        "I_MN = [NestedEstimator(t_1, t_2, K, S_0, sigma, M, N) for _ in range(k)]\n",
        "\n",
        "# Estimate the RMSE\n",
        "rmse = np.sqrt(np.mean([(benchmark_value - sample)**2 for sample in I_MN]))\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Plot the histogram of the k samples\n",
        "plt.hist(I_MN,  bins=20)\n",
        "plt.title(f\"Histogram of Nested Monte-Carlo estimator with M and N\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that even with both cases we got good results, we can notice that whenever we increase of M and N, it leads to a better accuracy. However, the computational complexity increases as well."
      ],
      "metadata": {
        "id": "OzfQcxz9iYys"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCGgxonLuqGc"
      },
      "source": [
        "## Tasks:\n",
        "Use a deep neural network to approximate the inner conditional expectation in order to compute  this same price. Compare its efficiency with the above Nested Monte Carlo. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the project, we will be estimating $I$ as well but with a deep neural network. We start first by generating some data."
      ],
      "metadata": {
        "id": "LUaK-8y6oUhv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33iM16PEBSHa"
      },
      "source": [
        "##Generating Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by initializing our parameters with the number of generated values where we train 80% of this number."
      ],
      "metadata": {
        "id": "USFmpyBOo5Va"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML7T7OoUlcoC"
      },
      "outputs": [],
      "source": [
        "t_0 = 0\n",
        "t_1 = 1\n",
        "T = 2    \n",
        "K = 100\n",
        "sigma = 0.3\n",
        "\n",
        "nb = 5000      # number of generated values\n",
        "train_nb = int(0.8*nb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we randomly generate values of $S_0$ from the uniform distribution do create a dataset for our NN to learn."
      ],
      "metadata": {
        "id": "H319so0zPl-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S_0 = np.array(np.random.uniform(30, 300, nb), dtype='float32').reshape(nb, 1) # stock price as an array of nb random values uniformly distributed between 30 and 300."
      ],
      "metadata": {
        "id": "UUJfwvGTPjWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the put price for all the generated $S_0$ at time $t_0$ correspondingly. "
      ],
      "metadata": {
        "id": "Ev4LK1ICP0DQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KcujOXat2ef"
      },
      "outputs": [],
      "source": [
        "price_at_zero = putPrice(t=0, S=S_0, T=t_2, K=K, sigma=sigma) #put price at t=0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with the results below, we find the real values of the integral $I$, which we are going to set as a target values for our future NN. Unfortunately, even with GPU it takes quite long to calculate the integral values, so we were not able to construct a big dataset, that obviously influences our results in the future. "
      ],
      "metadata": {
        "id": "iQ-qnVIHpxkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQKQSeKujcsP",
        "outputId": "03e26f4e-74c5-472c-9e21-101636140d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-33bdfd948258>:8: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  d = np.log(K/S) / sigmaSqrtDeltaT + sigmaSqrtDeltaT/2\n",
            "<ipython-input-7-33bdfd948258>:8: RuntimeWarning: overflow encountered in true_divide\n",
            "  d = np.log(K/S) / sigmaSqrtDeltaT + sigmaSqrtDeltaT/2\n",
            "<ipython-input-15-e0a0a429a7d7>:16: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
            "  the requested tolerance from being achieved.  The error may be \n",
            "  underestimated.\n",
            "  benchmark_value.append(quad(to_integrate, a= -np.inf, b =np.inf)[0])\n",
            "<ipython-input-15-e0a0a429a7d7>:16: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
            "  If increasing the limit yields no improvement it is advised to analyze \n",
            "  the integrand in order to determine the difficulties.  If the position of a \n",
            "  local difficulty can be determined (singularity, discontinuity) one will \n",
            "  probably gain from splitting up the interval and calling the integrator \n",
            "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
            "  benchmark_value.append(quad(to_integrate, a= -np.inf, b =np.inf)[0])\n"
          ]
        }
      ],
      "source": [
        "benchmark_value = []\n",
        "\n",
        "for i in range(len(S_0)):\n",
        "    def to_integrate(y, sigma = 0.3, S_0 = S_0[i], t_0 = 0, t_1 = 1, price_at_zero = price_at_zero[i]):\n",
        "        \n",
        "        S_t_1 = S_0 * np.exp(sigma*np.sqrt(t_1) * y - 0.5*sigma*sigma*t_1)\n",
        "        \n",
        "        loss_t_1 = putPrice(t=t_1, S=S_t_1, T=t_2, K=K, sigma=sigma) - price_at_zero\n",
        "\n",
        "        positivePart = np.maximum(loss_t_1, 0.)\n",
        "\n",
        "        density = np.exp(-0.5*y*y) / np.sqrt(2*np.pi)\n",
        "\n",
        "        return positivePart * density\n",
        "\n",
        "    benchmark_value.append(quad(to_integrate, a= -np.inf, b =np.inf)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us show the dataset we decided to work with, the fixed and changable values. \n",
        "\n",
        "As we have said, we generated Stock Prices from the uniform distribution, and then found the Put Prices. We fixed the values of $t_0$, $t_1$, maturity time $T$ and volatility. \n",
        "\n",
        "The logic under this was next: \n",
        "\n",
        "*   We do not have enough data to make a lot of parameters changable;\n",
        "*   The Put price at $t_0, S_0$ is the value that is quite easy to find, and it is directly related to the explicit integral formula of $I$, that we took as a benchmark value; consequently we can expect the NN to catch the relevant trens and patterns and give us a nice output;\n",
        "* Other values, that are present in the formula are hard to find without any additional information.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ykEVArf0qrvR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "omt_VmQCqWs8",
        "outputId": "3cacc434-99cb-4358-f806-ba4fd5a7b1c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Stock Price  t_0  t_1    T  Volatility  Put Price  I(target)\n",
              "0      106.145409  0.0  1.0  2.0         0.3  14.411290   4.895719\n",
              "1      290.468475  0.0  1.0  2.0         0.3   0.136451   0.103866\n",
              "2      277.238007  0.0  1.0  2.0         0.3   0.186684   0.139127\n",
              "3       66.006439  0.0  1.0  2.0         0.3  36.939181   6.399679\n",
              "4      194.915604  0.0  1.0  2.0         0.3   1.442513   0.876286\n",
              "...           ...  ...  ...  ...         ...        ...        ...\n",
              "4995    40.642342  0.0  1.0  2.0         0.3  59.519256   4.756676\n",
              "4996    90.711784  0.0  1.0  2.0         0.3  21.077840   5.841697\n",
              "4997   226.752335  0.0  1.0  2.0         0.3   0.642265   0.430335\n",
              "4998    78.189377  0.0  1.0  2.0         0.3  28.269845   6.343529\n",
              "4999   242.613190  0.0  1.0  2.0         0.3   0.432770   0.301397\n",
              "\n",
              "[5000 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-622f94c1-4603-4f56-b037-73529b705400\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Stock Price</th>\n",
              "      <th>t_0</th>\n",
              "      <th>t_1</th>\n",
              "      <th>T</th>\n",
              "      <th>Volatility</th>\n",
              "      <th>Put Price</th>\n",
              "      <th>I(target)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>106.145409</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>14.411290</td>\n",
              "      <td>4.895719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>290.468475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.136451</td>\n",
              "      <td>0.103866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>277.238007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.186684</td>\n",
              "      <td>0.139127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>66.006439</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>36.939181</td>\n",
              "      <td>6.399679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>194.915604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.442513</td>\n",
              "      <td>0.876286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>40.642342</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>59.519256</td>\n",
              "      <td>4.756676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>90.711784</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>21.077840</td>\n",
              "      <td>5.841697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>226.752335</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.642265</td>\n",
              "      <td>0.430335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>78.189377</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>28.269845</td>\n",
              "      <td>6.343529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>242.613190</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.432770</td>\n",
              "      <td>0.301397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-622f94c1-4603-4f56-b037-73529b705400')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-622f94c1-4603-4f56-b037-73529b705400 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-622f94c1-4603-4f56-b037-73529b705400');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(S_0)\n",
        "df.columns = ['Stock Price']\n",
        "df['t_0'] = t_0 * np.ones_like(S_0, dtype=float)\n",
        "df['t_1'] = t_1 * np.ones_like(S_0, dtype=float)\n",
        "df['T'] = T * np.ones_like(S_0, dtype=float)\n",
        "df['Volatility'] = sigma * np.ones_like(S_0, dtype=float)\n",
        "df['Put Price'] = putPrice(t=t_0, S=S_0, T=t_2, K=K, sigma=sigma)\n",
        "df['I(target)'] = benchmark_value\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBjJMoMcTO9"
      },
      "source": [
        "##Splitting the data into Train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the code, we assign to X and y the values of the put price and benchmark values from our dataframe and then we split the data into train and test sets."
      ],
      "metadata": {
        "id": "xz0uX-VurEcf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrSp2em79Ctm"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df['Put Price']\n",
        "y = df['I(target)']\n",
        "\n",
        "X_train = X[:train_nb].values\n",
        "y_train = y[:train_nb].values\n",
        "\n",
        "X_test = X[train_nb:].values\n",
        "y_test = y[train_nb:].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVBsDZ07ht7E"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eASlyZTtht7F"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LeakyReLU\n",
        "from keras import backend, Input\n",
        "from keras.constraints import non_neg\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "early_stopping=EarlyStopping(monitor='val_loss', patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEZKYH_uht7F"
      },
      "source": [
        "## Set up, compile and fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the models that we decided to use had the following characteristics:\n",
        "\n",
        "\n",
        "*   **Input**: Put Price\n",
        "*   **Output**: $I$ values\n",
        "*   **Loss function**: RMSE (since this is the one that we defined as a comparaison metric)\n",
        "*   **Main layers of NN**: Dense()\n",
        "\n",
        "In the following steps you will see the best model, that we could build, and its results, but firstly we want to present the logic behind each step in building it, and mention all uncountable trials, that unfortunately gave worse resunts. \n",
        "\n",
        "Here is the planthat we followed:\n",
        "\n",
        "\n",
        "1.   Build a small NN (2-5 Dense layers, try different combinations of Dropout)\n",
        "2.   See the results, compare after the trials, choose the best model and start to tune all the parameters (amount of the neurons, activation functions, lerning rate, Dropout rate etc)\n",
        "3. After theese steps did not give the desirable results -> add more layers and repeat the steps 1. 2. and 3. until the results that will satisfy. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZK3dVurUEu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We firstly define the loss function, that we will use for our NN:"
      ],
      "metadata": {
        "id": "RVWliYyXsjRi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBuOXXZn2__j"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create now our model which is a deep neural network with several hidden layers, each using the sigmoid activation function to introduce non-linearity. Dropout layers are added to reduce overfitting, and the final output layer predicts a single numerical value with a ReLU activation function. \n",
        "\n",
        "For more precision, the input layer is added with a shape of (1,), indicating a single input feature. Several dense layers with different activation functions (sigmoid) and different numbers of units (neurons) are added. The Dropout layers are added after some dense layers to prevent overfitting by randomly dropping a fraction of the input units during training. And the final layer is a dense layer with a single unit and the 'relu' activation function.\n",
        "A LeakyReLU activation function is added after the final layer to introduce non-linearity and prevent the dying ReLU problem.\n",
        "\n",
        "After creation of the model, we compile the model by specifying the optimizer, loss function, and optional metrics. The Adam optimizer is used with a learning rate of 0.00001 and the root_mean_squared_error function is set as the loss function. And then, we train the model for a specified number of epochs (500) with a batch size of 32. A validation split of 0.1 is used, which means 10% of the training data is used for validation during training.\n",
        "\n",
        "Finally, we use the trained model  to make predictions on the test data X_test."
      ],
      "metadata": {
        "id": "O1EOUqwotcs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZEwnhbbht7G",
        "outputId": "3eb0ee26-22aa-4f03-a517-e8fbd5daed8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "113/113 [==============================] - 5s 6ms/step - loss: 2.7183 - val_loss: 2.7242\n",
            "Epoch 2/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.7135 - val_loss: 2.7143\n",
            "Epoch 3/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.7061 - val_loss: 2.7044\n",
            "Epoch 4/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.6928 - val_loss: 2.6946\n",
            "Epoch 5/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.6883 - val_loss: 2.6849\n",
            "Epoch 6/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.6810 - val_loss: 2.6751\n",
            "Epoch 7/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.6709 - val_loss: 2.6655\n",
            "Epoch 8/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.6583 - val_loss: 2.6559\n",
            "Epoch 9/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.6527 - val_loss: 2.6464\n",
            "Epoch 10/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.6470 - val_loss: 2.6369\n",
            "Epoch 11/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.6384 - val_loss: 2.6274\n",
            "Epoch 12/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.6257 - val_loss: 2.6182\n",
            "Epoch 13/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.6182 - val_loss: 2.6090\n",
            "Epoch 14/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.6084 - val_loss: 2.5999\n",
            "Epoch 15/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.6014 - val_loss: 2.5907\n",
            "Epoch 16/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.5932 - val_loss: 2.5817\n",
            "Epoch 17/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.5816 - val_loss: 2.5728\n",
            "Epoch 18/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.5750 - val_loss: 2.5640\n",
            "Epoch 19/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5687 - val_loss: 2.5552\n",
            "Epoch 20/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.5614 - val_loss: 2.5466\n",
            "Epoch 21/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5584 - val_loss: 2.5379\n",
            "Epoch 22/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5417 - val_loss: 2.5295\n",
            "Epoch 23/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5376 - val_loss: 2.5211\n",
            "Epoch 24/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5351 - val_loss: 2.5127\n",
            "Epoch 25/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5236 - val_loss: 2.5044\n",
            "Epoch 26/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5175 - val_loss: 2.4963\n",
            "Epoch 27/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5075 - val_loss: 2.4882\n",
            "Epoch 28/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.5087 - val_loss: 2.4801\n",
            "Epoch 29/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.4968 - val_loss: 2.4723\n",
            "Epoch 30/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.4922 - val_loss: 2.4645\n",
            "Epoch 31/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.4803 - val_loss: 2.4569\n",
            "Epoch 32/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.4756 - val_loss: 2.4494\n",
            "Epoch 33/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.4720 - val_loss: 2.4420\n",
            "Epoch 34/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.4609 - val_loss: 2.4348\n",
            "Epoch 35/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.4554 - val_loss: 2.4277\n",
            "Epoch 36/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.4524 - val_loss: 2.4206\n",
            "Epoch 37/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.4448 - val_loss: 2.4137\n",
            "Epoch 38/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.4402 - val_loss: 2.4068\n",
            "Epoch 39/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.4323 - val_loss: 2.4000\n",
            "Epoch 40/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.4233 - val_loss: 2.3936\n",
            "Epoch 41/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.4204 - val_loss: 2.3871\n",
            "Epoch 42/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.4139 - val_loss: 2.3808\n",
            "Epoch 43/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.4123 - val_loss: 2.3745\n",
            "Epoch 44/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.4042 - val_loss: 2.3685\n",
            "Epoch 45/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3976 - val_loss: 2.3626\n",
            "Epoch 46/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3936 - val_loss: 2.3568\n",
            "Epoch 47/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.3897 - val_loss: 2.3510\n",
            "Epoch 48/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.3854 - val_loss: 2.3455\n",
            "Epoch 49/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3801 - val_loss: 2.3400\n",
            "Epoch 50/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3774 - val_loss: 2.3346\n",
            "Epoch 51/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3692 - val_loss: 2.3295\n",
            "Epoch 52/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3723 - val_loss: 2.3243\n",
            "Epoch 53/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3642 - val_loss: 2.3192\n",
            "Epoch 54/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3600 - val_loss: 2.3144\n",
            "Epoch 55/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3565 - val_loss: 2.3097\n",
            "Epoch 56/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3533 - val_loss: 2.3050\n",
            "Epoch 57/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3493 - val_loss: 2.3006\n",
            "Epoch 58/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3442 - val_loss: 2.2960\n",
            "Epoch 59/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3421 - val_loss: 2.2917\n",
            "Epoch 60/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3380 - val_loss: 2.2876\n",
            "Epoch 61/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3338 - val_loss: 2.2834\n",
            "Epoch 62/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.3292 - val_loss: 2.2795\n",
            "Epoch 63/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.3278 - val_loss: 2.2756\n",
            "Epoch 64/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.3271 - val_loss: 2.2718\n",
            "Epoch 65/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.3226 - val_loss: 2.2683\n",
            "Epoch 66/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3200 - val_loss: 2.2646\n",
            "Epoch 67/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3198 - val_loss: 2.2612\n",
            "Epoch 68/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3137 - val_loss: 2.2578\n",
            "Epoch 69/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3095 - val_loss: 2.2546\n",
            "Epoch 70/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3096 - val_loss: 2.2514\n",
            "Epoch 71/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3048 - val_loss: 2.2484\n",
            "Epoch 72/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3036 - val_loss: 2.2455\n",
            "Epoch 73/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.3029 - val_loss: 2.2426\n",
            "Epoch 74/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2983 - val_loss: 2.2399\n",
            "Epoch 75/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2967 - val_loss: 2.2372\n",
            "Epoch 76/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2959 - val_loss: 2.2346\n",
            "Epoch 77/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2956 - val_loss: 2.2321\n",
            "Epoch 78/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2924 - val_loss: 2.2297\n",
            "Epoch 79/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2936 - val_loss: 2.2274\n",
            "Epoch 80/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2886 - val_loss: 2.2251\n",
            "Epoch 81/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2870 - val_loss: 2.2230\n",
            "Epoch 82/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2861 - val_loss: 2.2209\n",
            "Epoch 83/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2833 - val_loss: 2.2189\n",
            "Epoch 84/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2849 - val_loss: 2.2169\n",
            "Epoch 85/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2796 - val_loss: 2.2151\n",
            "Epoch 86/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2775 - val_loss: 2.2133\n",
            "Epoch 87/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2805 - val_loss: 2.2116\n",
            "Epoch 88/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2747 - val_loss: 2.2099\n",
            "Epoch 89/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2781 - val_loss: 2.2083\n",
            "Epoch 90/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2765 - val_loss: 2.2067\n",
            "Epoch 91/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2760 - val_loss: 2.2052\n",
            "Epoch 92/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2733 - val_loss: 2.2040\n",
            "Epoch 93/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2731 - val_loss: 2.2026\n",
            "Epoch 94/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2712 - val_loss: 2.2014\n",
            "Epoch 95/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2724 - val_loss: 2.2001\n",
            "Epoch 96/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2708 - val_loss: 2.1990\n",
            "Epoch 97/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2700 - val_loss: 2.1979\n",
            "Epoch 98/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2694 - val_loss: 2.1969\n",
            "Epoch 99/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2681 - val_loss: 2.1959\n",
            "Epoch 100/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2683 - val_loss: 2.1949\n",
            "Epoch 101/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2685 - val_loss: 2.1939\n",
            "Epoch 102/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2684 - val_loss: 2.1930\n",
            "Epoch 103/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2663 - val_loss: 2.1922\n",
            "Epoch 104/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2675 - val_loss: 2.1913\n",
            "Epoch 105/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2663 - val_loss: 2.1905\n",
            "Epoch 106/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2657 - val_loss: 2.1898\n",
            "Epoch 107/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2635 - val_loss: 2.1890\n",
            "Epoch 108/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2657 - val_loss: 2.1883\n",
            "Epoch 109/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2630 - val_loss: 2.1877\n",
            "Epoch 110/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2628 - val_loss: 2.1871\n",
            "Epoch 111/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2623 - val_loss: 2.1866\n",
            "Epoch 112/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2636 - val_loss: 2.1860\n",
            "Epoch 113/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2621 - val_loss: 2.1855\n",
            "Epoch 114/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2632 - val_loss: 2.1849\n",
            "Epoch 115/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2614 - val_loss: 2.1845\n",
            "Epoch 116/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2634 - val_loss: 2.1840\n",
            "Epoch 117/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2591 - val_loss: 2.1837\n",
            "Epoch 118/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2608 - val_loss: 2.1832\n",
            "Epoch 119/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2608 - val_loss: 2.1828\n",
            "Epoch 120/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2614 - val_loss: 2.1824\n",
            "Epoch 121/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2608 - val_loss: 2.1821\n",
            "Epoch 122/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2605 - val_loss: 2.1817\n",
            "Epoch 123/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1815\n",
            "Epoch 124/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1811\n",
            "Epoch 125/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1809\n",
            "Epoch 126/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2616 - val_loss: 2.1806\n",
            "Epoch 127/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2596 - val_loss: 2.1804\n",
            "Epoch 128/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2600 - val_loss: 2.1801\n",
            "Epoch 129/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2607 - val_loss: 2.1798\n",
            "Epoch 130/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2591 - val_loss: 2.1797\n",
            "Epoch 131/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2611 - val_loss: 2.1794\n",
            "Epoch 132/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2614 - val_loss: 2.1792\n",
            "Epoch 133/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2601 - val_loss: 2.1790\n",
            "Epoch 134/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2571 - val_loss: 2.1789\n",
            "Epoch 135/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2611 - val_loss: 2.1787\n",
            "Epoch 136/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2576 - val_loss: 2.1785\n",
            "Epoch 137/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2583 - val_loss: 2.1784\n",
            "Epoch 138/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2588 - val_loss: 2.1782\n",
            "Epoch 139/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2589 - val_loss: 2.1781\n",
            "Epoch 140/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2578 - val_loss: 2.1780\n",
            "Epoch 141/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1778\n",
            "Epoch 142/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1777\n",
            "Epoch 143/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2575 - val_loss: 2.1776\n",
            "Epoch 144/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2607 - val_loss: 2.1775\n",
            "Epoch 145/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1773\n",
            "Epoch 146/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1772\n",
            "Epoch 147/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2589 - val_loss: 2.1771\n",
            "Epoch 148/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2579 - val_loss: 2.1770\n",
            "Epoch 149/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2578 - val_loss: 2.1769\n",
            "Epoch 150/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2583 - val_loss: 2.1769\n",
            "Epoch 151/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2604 - val_loss: 2.1768\n",
            "Epoch 152/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2568 - val_loss: 2.1767\n",
            "Epoch 153/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2585 - val_loss: 2.1766\n",
            "Epoch 154/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2579 - val_loss: 2.1765\n",
            "Epoch 155/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2569 - val_loss: 2.1765\n",
            "Epoch 156/500\n",
            "113/113 [==============================] - 1s 9ms/step - loss: 2.2586 - val_loss: 2.1764\n",
            "Epoch 157/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2584 - val_loss: 2.1763\n",
            "Epoch 158/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2593 - val_loss: 2.1763\n",
            "Epoch 159/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2574 - val_loss: 2.1762\n",
            "Epoch 160/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2567 - val_loss: 2.1762\n",
            "Epoch 161/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1762\n",
            "Epoch 162/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1761\n",
            "Epoch 163/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2575 - val_loss: 2.1760\n",
            "Epoch 164/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2596 - val_loss: 2.1760\n",
            "Epoch 165/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1760\n",
            "Epoch 166/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1759\n",
            "Epoch 167/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1759\n",
            "Epoch 168/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1758\n",
            "Epoch 169/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2568 - val_loss: 2.1758\n",
            "Epoch 170/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2596 - val_loss: 2.1758\n",
            "Epoch 171/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2580 - val_loss: 2.1757\n",
            "Epoch 172/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2590 - val_loss: 2.1757\n",
            "Epoch 173/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2594 - val_loss: 2.1757\n",
            "Epoch 174/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1756\n",
            "Epoch 175/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1756\n",
            "Epoch 176/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1756\n",
            "Epoch 177/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1755\n",
            "Epoch 178/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1755\n",
            "Epoch 179/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2594 - val_loss: 2.1755\n",
            "Epoch 180/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2573 - val_loss: 2.1754\n",
            "Epoch 181/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1754\n",
            "Epoch 182/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2589 - val_loss: 2.1754\n",
            "Epoch 183/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2602 - val_loss: 2.1754\n",
            "Epoch 184/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2600 - val_loss: 2.1753\n",
            "Epoch 185/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2583 - val_loss: 2.1753\n",
            "Epoch 186/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2579 - val_loss: 2.1753\n",
            "Epoch 187/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2574 - val_loss: 2.1753\n",
            "Epoch 188/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2584 - val_loss: 2.1752\n",
            "Epoch 189/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2600 - val_loss: 2.1753\n",
            "Epoch 190/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2599 - val_loss: 2.1752\n",
            "Epoch 191/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2601 - val_loss: 2.1752\n",
            "Epoch 192/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2584 - val_loss: 2.1752\n",
            "Epoch 193/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2590 - val_loss: 2.1752\n",
            "Epoch 194/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2601 - val_loss: 2.1751\n",
            "Epoch 195/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1751\n",
            "Epoch 196/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2586 - val_loss: 2.1752\n",
            "Epoch 197/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2589 - val_loss: 2.1751\n",
            "Epoch 198/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2563 - val_loss: 2.1751\n",
            "Epoch 199/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2604 - val_loss: 2.1751\n",
            "Epoch 200/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2599 - val_loss: 2.1751\n",
            "Epoch 201/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2587 - val_loss: 2.1750\n",
            "Epoch 202/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2584 - val_loss: 2.1751\n",
            "Epoch 203/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2595 - val_loss: 2.1750\n",
            "Epoch 204/500\n",
            "113/113 [==============================] - 1s 9ms/step - loss: 2.2585 - val_loss: 2.1750\n",
            "Epoch 205/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2562 - val_loss: 2.1750\n",
            "Epoch 206/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2603 - val_loss: 2.1750\n",
            "Epoch 207/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1750\n",
            "Epoch 208/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1750\n",
            "Epoch 209/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2563 - val_loss: 2.1750\n",
            "Epoch 210/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1750\n",
            "Epoch 211/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2578 - val_loss: 2.1750\n",
            "Epoch 212/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2582 - val_loss: 2.1750\n",
            "Epoch 213/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2598 - val_loss: 2.1750\n",
            "Epoch 214/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2605 - val_loss: 2.1750\n",
            "Epoch 215/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2574 - val_loss: 2.1750\n",
            "Epoch 216/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2580 - val_loss: 2.1750\n",
            "Epoch 217/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2591 - val_loss: 2.1750\n",
            "Epoch 218/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2582 - val_loss: 2.1750\n",
            "Epoch 219/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2588 - val_loss: 2.1749\n",
            "Epoch 220/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2567 - val_loss: 2.1749\n",
            "Epoch 221/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2586 - val_loss: 2.1749\n",
            "Epoch 222/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1749\n",
            "Epoch 223/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1749\n",
            "Epoch 224/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2578 - val_loss: 2.1749\n",
            "Epoch 225/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2591 - val_loss: 2.1750\n",
            "Epoch 226/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2581 - val_loss: 2.1749\n",
            "Epoch 227/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2603 - val_loss: 2.1749\n",
            "Epoch 228/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2585 - val_loss: 2.1749\n",
            "Epoch 229/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2592 - val_loss: 2.1749\n",
            "Epoch 230/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1749\n",
            "Epoch 231/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2562 - val_loss: 2.1749\n",
            "Epoch 232/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1749\n",
            "Epoch 233/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2592 - val_loss: 2.1749\n",
            "Epoch 234/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2573 - val_loss: 2.1749\n",
            "Epoch 235/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2574 - val_loss: 2.1749\n",
            "Epoch 236/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1749\n",
            "Epoch 237/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1749\n",
            "Epoch 238/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2590 - val_loss: 2.1749\n",
            "Epoch 239/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2586 - val_loss: 2.1749\n",
            "Epoch 240/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2582 - val_loss: 2.1749\n",
            "Epoch 241/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2599 - val_loss: 2.1749\n",
            "Epoch 242/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2579 - val_loss: 2.1749\n",
            "Epoch 243/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1749\n",
            "Epoch 244/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2590 - val_loss: 2.1749\n",
            "Epoch 245/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1749\n",
            "Epoch 246/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2586 - val_loss: 2.1749\n",
            "Epoch 247/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2596 - val_loss: 2.1749\n",
            "Epoch 248/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2609 - val_loss: 2.1749\n",
            "Epoch 249/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2577 - val_loss: 2.1749\n",
            "Epoch 250/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 251/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 252/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 253/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 254/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 255/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 256/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 257/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2574 - val_loss: 2.1748\n",
            "Epoch 258/500\n",
            "113/113 [==============================] - 1s 9ms/step - loss: 2.2590 - val_loss: 2.1748\n",
            "Epoch 259/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 260/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 261/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2574 - val_loss: 2.1748\n",
            "Epoch 262/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 263/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 264/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 265/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2569 - val_loss: 2.1748\n",
            "Epoch 266/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2584 - val_loss: 2.1748\n",
            "Epoch 267/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2575 - val_loss: 2.1748\n",
            "Epoch 268/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2583 - val_loss: 2.1749\n",
            "Epoch 269/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2588 - val_loss: 2.1749\n",
            "Epoch 270/500\n",
            "113/113 [==============================] - 1s 9ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 271/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2568 - val_loss: 2.1748\n",
            "Epoch 272/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2592 - val_loss: 2.1749\n",
            "Epoch 273/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2567 - val_loss: 2.1749\n",
            "Epoch 274/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2572 - val_loss: 2.1749\n",
            "Epoch 275/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2576 - val_loss: 2.1749\n",
            "Epoch 276/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2578 - val_loss: 2.1749\n",
            "Epoch 277/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1749\n",
            "Epoch 278/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1749\n",
            "Epoch 279/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1748\n",
            "Epoch 280/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2598 - val_loss: 2.1749\n",
            "Epoch 281/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1749\n",
            "Epoch 282/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 283/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2574 - val_loss: 2.1748\n",
            "Epoch 284/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 285/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2555 - val_loss: 2.1748\n",
            "Epoch 286/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1749\n",
            "Epoch 287/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1749\n",
            "Epoch 288/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 289/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1749\n",
            "Epoch 290/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1749\n",
            "Epoch 291/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2594 - val_loss: 2.1749\n",
            "Epoch 292/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 293/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2565 - val_loss: 2.1748\n",
            "Epoch 294/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2584 - val_loss: 2.1748\n",
            "Epoch 295/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2581 - val_loss: 2.1748\n",
            "Epoch 296/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 297/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2576 - val_loss: 2.1748\n",
            "Epoch 298/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2571 - val_loss: 2.1749\n",
            "Epoch 299/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1748\n",
            "Epoch 300/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2602 - val_loss: 2.1748\n",
            "Epoch 301/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2576 - val_loss: 2.1749\n",
            "Epoch 302/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1749\n",
            "Epoch 303/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1748\n",
            "Epoch 304/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2590 - val_loss: 2.1748\n",
            "Epoch 305/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 306/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2608 - val_loss: 2.1748\n",
            "Epoch 307/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2573 - val_loss: 2.1748\n",
            "Epoch 308/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2598 - val_loss: 2.1748\n",
            "Epoch 309/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2593 - val_loss: 2.1748\n",
            "Epoch 310/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2603 - val_loss: 2.1748\n",
            "Epoch 311/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2573 - val_loss: 2.1748\n",
            "Epoch 312/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 313/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 314/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2581 - val_loss: 2.1748\n",
            "Epoch 315/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2576 - val_loss: 2.1748\n",
            "Epoch 316/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2575 - val_loss: 2.1748\n",
            "Epoch 317/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 318/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 319/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 320/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2599 - val_loss: 2.1748\n",
            "Epoch 321/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 322/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 323/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2562 - val_loss: 2.1748\n",
            "Epoch 324/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2569 - val_loss: 2.1748\n",
            "Epoch 325/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1748\n",
            "Epoch 326/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2576 - val_loss: 2.1748\n",
            "Epoch 327/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 328/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2592 - val_loss: 2.1748\n",
            "Epoch 329/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 330/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 331/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2610 - val_loss: 2.1748\n",
            "Epoch 332/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2568 - val_loss: 2.1748\n",
            "Epoch 333/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1748\n",
            "Epoch 334/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 335/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2594 - val_loss: 2.1748\n",
            "Epoch 336/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 337/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 338/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2573 - val_loss: 2.1748\n",
            "Epoch 339/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 340/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2573 - val_loss: 2.1748\n",
            "Epoch 341/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 342/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 343/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2574 - val_loss: 2.1748\n",
            "Epoch 344/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 345/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2590 - val_loss: 2.1748\n",
            "Epoch 346/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 347/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2590 - val_loss: 2.1747\n",
            "Epoch 348/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2565 - val_loss: 2.1748\n",
            "Epoch 349/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2591 - val_loss: 2.1748\n",
            "Epoch 350/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 351/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 352/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 353/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2558 - val_loss: 2.1748\n",
            "Epoch 354/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2596 - val_loss: 2.1748\n",
            "Epoch 355/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2600 - val_loss: 2.1748\n",
            "Epoch 356/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 357/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2564 - val_loss: 2.1748\n",
            "Epoch 358/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2596 - val_loss: 2.1748\n",
            "Epoch 359/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2565 - val_loss: 2.1748\n",
            "Epoch 360/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2577 - val_loss: 2.1748\n",
            "Epoch 361/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2596 - val_loss: 2.1748\n",
            "Epoch 362/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 363/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 364/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2596 - val_loss: 2.1748\n",
            "Epoch 365/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 366/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 367/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2590 - val_loss: 2.1748\n",
            "Epoch 368/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 369/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1748\n",
            "Epoch 370/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1748\n",
            "Epoch 371/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2598 - val_loss: 2.1748\n",
            "Epoch 372/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2581 - val_loss: 2.1748\n",
            "Epoch 373/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 374/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 375/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 376/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2607 - val_loss: 2.1748\n",
            "Epoch 377/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2596 - val_loss: 2.1748\n",
            "Epoch 378/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 379/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2575 - val_loss: 2.1748\n",
            "Epoch 380/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 381/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 382/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2599 - val_loss: 2.1748\n",
            "Epoch 383/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2571 - val_loss: 2.1748\n",
            "Epoch 384/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2584 - val_loss: 2.1748\n",
            "Epoch 385/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 386/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2584 - val_loss: 2.1748\n",
            "Epoch 387/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2601 - val_loss: 2.1748\n",
            "Epoch 388/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2572 - val_loss: 2.1748\n",
            "Epoch 389/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2573 - val_loss: 2.1748\n",
            "Epoch 390/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 391/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2591 - val_loss: 2.1747\n",
            "Epoch 392/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2592 - val_loss: 2.1747\n",
            "Epoch 393/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2597 - val_loss: 2.1747\n",
            "Epoch 394/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 395/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2603 - val_loss: 2.1747\n",
            "Epoch 396/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2589 - val_loss: 2.1747\n",
            "Epoch 397/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2566 - val_loss: 2.1747\n",
            "Epoch 398/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2594 - val_loss: 2.1748\n",
            "Epoch 399/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 400/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2575 - val_loss: 2.1748\n",
            "Epoch 401/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 402/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2600 - val_loss: 2.1748\n",
            "Epoch 403/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2605 - val_loss: 2.1748\n",
            "Epoch 404/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2563 - val_loss: 2.1748\n",
            "Epoch 405/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2599 - val_loss: 2.1748\n",
            "Epoch 406/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 407/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 408/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2572 - val_loss: 2.1748\n",
            "Epoch 409/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2576 - val_loss: 2.1748\n",
            "Epoch 410/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2576 - val_loss: 2.1748\n",
            "Epoch 411/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 412/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2576 - val_loss: 2.1748\n",
            "Epoch 413/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2573 - val_loss: 2.1748\n",
            "Epoch 414/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 415/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 416/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2598 - val_loss: 2.1748\n",
            "Epoch 417/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 418/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 419/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 420/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 421/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2592 - val_loss: 2.1748\n",
            "Epoch 422/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2606 - val_loss: 2.1748\n",
            "Epoch 423/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2569 - val_loss: 2.1748\n",
            "Epoch 424/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2584 - val_loss: 2.1748\n",
            "Epoch 425/500\n",
            "113/113 [==============================] - 1s 9ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 426/500\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 2.2605 - val_loss: 2.1748\n",
            "Epoch 427/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2567 - val_loss: 2.1748\n",
            "Epoch 428/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2590 - val_loss: 2.1748\n",
            "Epoch 429/500\n",
            "113/113 [==============================] - 1s 7ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 430/500\n",
            "113/113 [==============================] - 1s 11ms/step - loss: 2.2598 - val_loss: 2.1748\n",
            "Epoch 431/500\n",
            "113/113 [==============================] - 1s 11ms/step - loss: 2.2591 - val_loss: 2.1748\n",
            "Epoch 432/500\n",
            "113/113 [==============================] - 1s 12ms/step - loss: 2.2592 - val_loss: 2.1748\n",
            "Epoch 433/500\n",
            "113/113 [==============================] - 1s 10ms/step - loss: 2.2574 - val_loss: 2.1748\n",
            "Epoch 434/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 435/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2592 - val_loss: 2.1748\n",
            "Epoch 436/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2568 - val_loss: 2.1748\n",
            "Epoch 437/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2593 - val_loss: 2.1748\n",
            "Epoch 438/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 439/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2600 - val_loss: 2.1748\n",
            "Epoch 440/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2591 - val_loss: 2.1748\n",
            "Epoch 441/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2599 - val_loss: 2.1748\n",
            "Epoch 442/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2580 - val_loss: 2.1747\n",
            "Epoch 443/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2575 - val_loss: 2.1748\n",
            "Epoch 444/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2600 - val_loss: 2.1748\n",
            "Epoch 445/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2567 - val_loss: 2.1748\n",
            "Epoch 446/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 447/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2571 - val_loss: 2.1747\n",
            "Epoch 448/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2599 - val_loss: 2.1748\n",
            "Epoch 449/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 450/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 451/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 452/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2570 - val_loss: 2.1748\n",
            "Epoch 453/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2569 - val_loss: 2.1748\n",
            "Epoch 454/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2578 - val_loss: 2.1748\n",
            "Epoch 455/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 456/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2603 - val_loss: 2.1748\n",
            "Epoch 457/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1748\n",
            "Epoch 458/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2566 - val_loss: 2.1748\n",
            "Epoch 459/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 460/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2593 - val_loss: 2.1748\n",
            "Epoch 461/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 462/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2564 - val_loss: 2.1748\n",
            "Epoch 463/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 464/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2590 - val_loss: 2.1748\n",
            "Epoch 465/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2597 - val_loss: 2.1748\n",
            "Epoch 466/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2602 - val_loss: 2.1748\n",
            "Epoch 467/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1748\n",
            "Epoch 468/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 469/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2581 - val_loss: 2.1748\n",
            "Epoch 470/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 471/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2572 - val_loss: 2.1748\n",
            "Epoch 472/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 473/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2583 - val_loss: 2.1748\n",
            "Epoch 474/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "Epoch 475/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 476/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2565 - val_loss: 2.1748\n",
            "Epoch 477/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2561 - val_loss: 2.1748\n",
            "Epoch 478/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 479/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2590 - val_loss: 2.1749\n",
            "Epoch 480/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2594 - val_loss: 2.1748\n",
            "Epoch 481/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 482/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 483/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 484/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2592 - val_loss: 2.1748\n",
            "Epoch 485/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2585 - val_loss: 2.1748\n",
            "Epoch 486/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2579 - val_loss: 2.1748\n",
            "Epoch 487/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2588 - val_loss: 2.1748\n",
            "Epoch 488/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2576 - val_loss: 2.1749\n",
            "Epoch 489/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2587 - val_loss: 2.1749\n",
            "Epoch 490/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2581 - val_loss: 2.1749\n",
            "Epoch 491/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 492/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2600 - val_loss: 2.1748\n",
            "Epoch 493/500\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 2.2586 - val_loss: 2.1748\n",
            "Epoch 494/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2591 - val_loss: 2.1748\n",
            "Epoch 495/500\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 2.2595 - val_loss: 2.1748\n",
            "Epoch 496/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2587 - val_loss: 2.1748\n",
            "Epoch 497/500\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 2.2599 - val_loss: 2.1748\n",
            "Epoch 498/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2582 - val_loss: 2.1748\n",
            "Epoch 499/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2589 - val_loss: 2.1748\n",
            "Epoch 500/500\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 2.2580 - val_loss: 2.1748\n",
            "32/32 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "import keras \n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,)))\n",
        "model.add(Dense(512,  activation='sigmoid')) \n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(512,  activation='sigmoid')) \n",
        "model.add(Dense(256,  activation='sigmoid')) \n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(256,  activation='sigmoid')) \n",
        "model.add(Dense(128,  activation='sigmoid')) \n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128,  activation='sigmoid')) \n",
        "model.add(Dense(64,  activation='sigmoid')) \n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(64,  activation='sigmoid')) \n",
        "model.add(Dense(32,  activation='sigmoid')) \n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,  activation='sigmoid')) \n",
        "model.add(Dense(16,  activation='sigmoid')) \n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16,  activation='sigmoid')) \n",
        "model.add(Dense(8,  activation='sigmoid')) \n",
        "opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "\n",
        "\n",
        "model.add(Dense(1 ,activation='relu'))\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "\n",
        "model.compile(optimizer = opt, loss = root_mean_squared_error)\n",
        "              \n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=500, validation_split=0.1)\n",
        "predictions=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the model, we use the mean squared error (MSE) between the predicted values and the true values y_test."
      ],
      "metadata": {
        "id": "0QDZ-sapwIE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u34DugUn9ySM",
        "outputId": "c340e4d3-867c-43ad-aa59-3de2c1f20d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error: 2.2622094\n"
          ]
        }
      ],
      "source": [
        "mse = tf.keras.losses.mean_squared_error(y_test, predictions).numpy()\n",
        "rmse = np.sqrt(np.mean(mse))\n",
        "print(\"Root Mean Squared Error:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "yswtregcGeSc",
        "outputId": "8ff2b847-3219-4a53-b659-3aaa26a0f2d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train','validation'],loc= 'upper left')\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the NN here coul not beat the Monte Carlo method with its precision of RMSE and we think there might be some reasons: \n",
        "\n",
        "\n",
        "*   Not enough of data. As you cna see, the maximum amount of neurons we took in the first Dense layer is 512, because after taking more, the modeel was automatically capturing irrelevant patterns from the data and giving worse results. \n",
        "*   Quite complicated function restore from the amount of input values and input data. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Nevertheless, we still think, that NN showed quite good results, and it potentially has a chance to beat MC if the issues described before are solved. "
      ],
      "metadata": {
        "id": "PohZHVnMy1qY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}